Docker Swarm and Kubernetes are both container orchestration platforms, but they have different strengths and use cases

Docker Swarm is easier to use and is better for smaller applications, while Kubernetes is more robust and better for large-scale applications.

Docker Swarm is Docker’s native clustering and orchestration tool, designed to manage and scale containerized applications.

While it provides simplicity and integration with Docker, it has several limitations compared to other orchestration solutions like Kubernetes.



Limitations
-----------

Docker Swarm is generally considered less scalable than Kubernetes. While it can handle clusters with a reasonable number of nodes and services, Kubernetes is designed for larger-scale deployments and can manage thousands of nodes and pods.

Swarm provides basic load balancing and routing but lacks the advanced capabilities like provided by K8S


Kubernetes offers more granular security controls, including Role-Based Access Control (RBAC), network policies, and security policies. Docker Swarm’s security features are less comprehensive in comparison.

Docker Swarm has a smaller community and less industry adoption compared to Kubernetes

Docker carries containers as shown in logo
And those are carried by K8S as it is a steering for docker with containers

Docker is used to create containers, while Kubernetes is used to manage them

DockerSwarm                  Kubernetes
===========                  ==========
Cluster                            Cluster
Nodes                             Nodes
Containers                     PODS
Applications                  Containers
                                        Applications



                                   ====================================================================
                                                                  Kubernetes
                                   ====================================================================

Kubernetes is a powerful open-source platform for automating the deployment, scaling, and operation of application containers.

Kubernetes is a Container Orchestration tool

Orchestration is nothing but Cluster

Kubernetes creates Cluster , deploy and manage clusters. Cluster is combination of 1 master and multiple nodes / Minion

Kubernetes master will not take the load, it will distribute load to Nodes / Slaves

Smallest object that Kubernetes can create is POD. WithIn POD, we have Container

Cluster is a bunch of PODS and POD contains docker containers

By using Kubernetes we form a cluster (group of PODS with docker containers)

K8S schedules, runs and manage isolated PODS

Kubernetes does not understand Containers

Kubernetes can understand only PODS




Features of Kubernetes
---------------------

Orchestration -- Clustering any number of containers on different hardwares / nodes
Autoscaling
Auto Healing -- New containers will create in case of containers crash. Same like Docker Swarm
Load Balancing
Roll Back  - Going to Previous Versions




Here's a brief history of Kubernetes, from its origins to its current state:

Origins and Predecessors
========================

Google's Borg:
-------------

The origins of Kubernetes can be traced back to Google's internal systems, particularly a system called Borg. Borg was a large-scale cluster management system that Google developed internally to manage its vast infrastructure. It allowed Google to deploy, manage, and scale applications across thousands of servers efficiently.

Omega:
-----

After Borg, Google developed Omega, which was an evolution of Borg. Omega introduced a more flexible and modular architecture. It further refined the ideas of scheduling, resource management, and container orchestration.

The Birth of Kubernetes
=======================

2014 - Project Launch:
---------------------

Kubernetes was officially announced by Google in mid-2014. It was initially developed by Google engineers Joe Beda, Brendan Burns, and Craig McLuckie.

Kubernetes was designed based on the lessons learned from Borg and Omega, but it was intended to be open-source and available to the broader community.

Open-Source and Community Involvement:
--------------------------------------

From the start, Kubernetes was designed as an open-source project, allowing developers from outside Google to contribute. It was released under the Apache 2.0 license, making it freely available for anyone to use and modify.


Partnership with the Cloud Native Computing Foundation (CNCF):
--------------------------------------------------------------

In 2015, Kubernetes was donated to the CNCF, which was formed to foster and sustain the development of cloud-native technologies. This move helped Kubernetes gain significant traction in the open-source community and among enterprises.



**************************************************************************************

Kubernetes Architecture
-----------------------

Kubernetes architecture is designed to manage containerized applications across a cluster of machines efficiently.

It follows a master-worker architecture where the master nodes control and manage the worker nodes.

1. Master Nodes  - Control Plane
2. Worker Nodes
3. PODS
4. Networking
5. Volumes
6. Namespaces
7. Services

MW- PNVNS


Master Nodes (Control Plane)
============================

The master node is responsible for managing the Kubernetes cluster.

It coordinates all activities within the cluster, including scheduling, scaling, and maintaining the desired state of the system.

Key Components of the Master Node:
------------------------------------

 1. API Server (kube-apiserver): The receptionist
    ----------------------------------------------

    The API server is the entry point for all REST commands used to control the cluster. communicate with user (takes command execute & give output).  kubectl is the command to communicate with API Server

 2. Scheduler (kube-scheduler): This will take Action
------------------------------------------------------
 
    The scheduler is responsible for placing the pods onto nodes within the cluster based on resource availability and other constraints. This Kube-Scheduler will communicate with the Nodes

 3. Controller Manager (kube-controller-manager):
---------------------------------------------------

    This component runs various controllers that regulate the state of the cluster.Node Controller (which handles node failures), Replication Controller (which maintains the correct number of pods), and more. It just control the k8s objects (n/w, service, Node)

 4. etcd
---------

    etcd is a key-value store used by Kubernetes to store all cluster data, including configuration data, state information, and metadata.  Database of the Cluster. It is a critical component, as it serves as the single source of truth for the cluster’s state.

 5. Cloud Controller Manager(Optional) :
---------------------------------------
 
    This component interacts with the underlying cloud infrastructure. It handles tasks like managing cloud resources (e.g., load balancers, volumes) in public or private cloud environments. It is responsible to make sure that the actual state is same as desired state.

In Short Cut
----------

MASTER:

1. API SERVER: Communicate with user (takes command execute & give op)
2. CONTROLLER: Control the k8s objects (n/w, service, Node)
3. SCHEDULER: Select the worker node to schedule pods (depends on hw of node)
4. ETCD: Database of cluster (stores complete info of a cluster ON KEY-VALUE pair)

ACSE

Worker Nodes
===============

Worker nodes are the machines where the actual workload, in the form of containerized applications, runs.

Each worker node contains several components that allow it to run and manage containers.

Key Components of the Worker Node:
---------------------------------

1. Kubelet:
   -------
The kubelet is an agent that runs on every worker node. It ensures that containers are running in a pod as defined by the Kubernetes API. It communicates with the API server on the master node and manages the lifecycle of the containers on its node.

Kubelet is the only component that will communicate with Master
   
2. Container Runtime:
   ------------------

This is the software responsible for running containers.

The most common runtime is Docker, but Kubernetes also supports others like containerd and CRI-O.

The container runtime pulls container images from a registry, unpacks them, and runs the application inside the containers.

3. Kube-Proxy:
   ----------

Kube-proxy is a network proxy that runs on each worker node. It maintains network rules that allow communication between pods and services within the cluster.  It enables the network routing and load balancing of traffic within the Kubernetes cluster.

Simply Kube-Proxy will Provide IP to the POD

4. PODS:
   -----

Pods are the smallest and simplest Kubernetes objects. A pod represents a single instance of a running process in your cluster and can contain one or more containers.

Pods are ephemeral, meaning they can be created and destroyed as needed, and are the unit of scaling in Kubernetes.


In Short Cut
------------

WORKER NODES:

1. KUBELET : Its an agent in worker node (it will inform all activities to master)
2. KUBEPROXY: It deals with Network (ip, networks, ports)
3. POD: Group of containers (inside pod we have application)

KCKP


Key Networking Concepts:
-----------------------

1. Cluster IP: An internal IP address that is assigned to a service within the cluster.

2. NodePort: Exposes a service on a static port on each node’s IP.

3. LoadBalancer: Provisions a load balancer in supported cloud environments to expose services externally.


Volumes
-------

Kubernetes Volumes provide persistent storage to containers within pods, allowing data to persist across pod restarts.

They can be backed by various storage backends, including local storage, cloud storage like AWS EBS

Namespaces
----------

Namespaces are a way to divide cluster resources between multiple users.

They provide a scope for names, meaning you can have multiple resources with the same name in different namespaces.

Services
--------

Services are used to access our applications which is on the pods using Cluster IP(internal access) or Load Balancer (external access)



K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION

In Kubernetes we need to Create Cluster. Cluster can create in 2 ways:  1. Self Managed 2. Cloud Based

CLUSTER TYPES:
-------------

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)

2. CLOUD-BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKS = GOOGLE KUBERENETS SERVICE


Kubectl is the command line tool for Kubernetes

If we want to execute commands we need to use kubectl.

MINIKUBE:
---------

It is a tool used to setup single node cluster on K8's.

Here Master and worker runs on same machine

It contains API Servers, ETDC database and container runtime

It is used for development, testing, and experimentation purposes on local.

NOTE: But we don't implement this in real-time Prod


Minikube Setup
--------------

Requirement
------------

Launch Ubuntu 24 with t2.small having min 8GB volume

Install minikube and Kubectl

vi minikube.sh

sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO  https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO " https://dl.k8s.io/release/$(curl -L -s  https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;
sudo curl -LO " https://dl.k8s.io/$(curl -L -s  https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256&quot;
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

Alternate

sudo apt update -y
sudo apt upgrade -y

sudo apt install curl wget apt-transport-https -y

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Install Minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
minikube version

# Install kubectl
KUBECTL_VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl.sha256"

# Verify the download
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check

# Install kubectl binary
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Start Minikube with Docker driver
sudo minikube start --driver=docker --force


KUBECTL:
--------
kubectl is the CLI which is used to interact with a Kubernetes cluster.

We can create, manage pods, services, deployments, and other resources.

The configuration of kubectl is in the $HOME/.kube directory.

-- minikube status

-- kubectl get nodes

-- kubectl get pods

PODS :
=======

It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.

We can create this pod in two ways,

1. Imperative(command)

2. Declarative (Manifest file) - can be reuse - in real time we go with declarative

By default, one pod has one container, if required we can create, if you create multiple containers in a single pod, all containers inside the pods will share the same volume


1. Imperative(command)
-----------------------

Example:
--------

-- kubectl run pod1 --image nginx    [Creating  pod with name pod1 with image nginx]

-- kubectl get pods/pod/po           [To get the pods , can use pods/po/pods also]

-- kubectl get pod -o wide           [To get details about the pod]

-- kubectl describe pod pod1         [To get more details about the pod]

-- kubectl delete pod pod1           [To delete the pod]

Example 2:
---------

-- kubectl run reyaz --image nginx  [This will create a pod name reyaz , with httpd image ]

-  kubectl get pods

-- kubectl logs reyaz  

-- kubectl exec -it reyaz -- /bin/bash   [to connect to the pod inside]
   cd /usr/share/nginx/html
   cat index.html

-- kubectl delete pod reyaz   [This will delete the pod reyaz]


The above approach we don't do, because need to create PODS manually , so lets go for manifest declarative approach


================================================

2. Declarative (Manifest file)
-----------------------------

In manifest file we have these mandatory parameters

---------------------
apiVersion:
kind:
metadata:
spec:
---------------------

What is apiVersion?
-------------------

apiVersion: Specifies the API version used for the Deployment object. apps/v1 is the stable API version for managing deployments.
            Depending on Kubernetes object we want to create, there is a corresponding     code library we want to use.
            apiVersion refers to Code Library

Examples:
---------

POD : v1
Service: v1
NameSpace: v1
Secrets: v1
Replicaset: apps/v1
Deployment: apps/v1
jobs: batch/v1



kubectl api-versions

What is kind ?
--------------

Refers to Kubernetes object which we want to create.

Example:
--------
kind: Pod
kind: Deployment
kind: Service
kind: Ingress
kind: job

What is metadata?
----------------

Additional information about the Kubernetes object like name, labels etc

name: The name of the Deployment.
labels: Key-value pairs used for organizing and selecting objects


What is spec?
------------

Contains docker container related information like, image name, environment variables , port mapping etc

How many number of pods (Replica)
About container and that should run on which image
On which port it should expose
Labeling the entire deployment etc



Sample manifest file
--------------------

apiVersion: v1
kind: Pod  -- creating pod
metadata:
  name: pod1 -- name of the pod
spec:  -- specifications
  containers:
    - image: nginx  -- image  name
      name: cont1 -- container name

-----------------

vi pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

-- kubectl create -f pod.yml    [ To create a pod using manifest]

-- kubectl describe pod pod1    [ To describe pod ]

-- kubectl delete pod pod1      [ To delete the pod ]

DRAWBACK: once pod is deleted we can't retrieve the pod.

If any pod deleted, it is deleted, no HA, for HA use ReplicaSET also called RS

KUBECOLOR:

wget  https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
./kubecolor
chmod +x kubecolor
mv kubecolor /usr/local/bin/

kubecolor get po

-- kubectl create -f pod.yml  

-- kubecolor get po

======================================================================================================================================

                                                             

=======================================================================================================================================

 we created a pod with manifest file, but we deleted that pod.

If any pod deleted, it is deleted, no HA, for HA use ReplicaSET also called RS


=================
REPLICASET
=================

This is used for managing multiple replicas of pod to perform activities like load balancing and autoscaling

It will create replicas of same pod.

we can use same application with multiple pods.

Even if one pod is deleted, automatically it will create another pod. It has self healing mechanism

Depends on requirement we can scale the pods.

We create rs --> rs wil create pods

LABELS:
-------

As we are creating multiple pods with same application, all these pods have different names but how to group all of them as we have 1 application with multiple pods. So we can give a label to group them

Individual pods are difficult to manage because they have different names
so we can give a common label to group them and work with them together

SELECTOR
--------

It is used to select pods with same labels

For replicaset use apiversion as apps/v1

how to find the apiresources to write in manifest file

kubectl api-resources

vi replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: ib-rs    ---------- Name of the Replicaset
  labels:
    app: bank
spec:     ------------- this spec is for PODS
  replicas: 3   --------- how many number of pods
  selector:
    matchLabels:  -Ensures only pods with label app: bank are part of this Replicaset. if there is any pod with label bank, it will be a part of this replicaset
      app: bank
  template:            ------------ Ensures the pods get labeled as app: bank
    metadata:
      labels:
        app: bank
    spec:  ----------------- this spec is for containers
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest




apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: ib-rs
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:  
    metadata:
      labels:
        app: bank
    spec:  
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest



-- kubectl create -f replicaset.yml

-- kubectl get replicaset

       (or)

-- kubectl get rs

-- kubectl get rs -o wide  [This command will get more details about ReplciaSets]

-- kubectl describe rs ib-rs  [This will describe about internetbanking-rs ]

-- kubectl get pods

-- kubectl get pods --show-labels   [This will list the pods with Labels]

If you delete any pod, automatically new pod will be created, if you want to watch live, open another terminate and give.

-- kubectl get pods --watch

-- kubectl get pods

-- kubectl delete pods pod-id      [First new pod create and the existing pod will be deleted]

-- kubectl get pods --show-labels  [New pod got created, this is called ReplicaSet, if one pod delete , another pod will get created automatically]

-- kubectl get pods -l app=bank  [This will list all the pods with label bank, l = label]

-- kubectl delete pods -l app=bank  [To delete all the pods wit label bank]

Note: Replicaset will take image details from manifest file -- replicaset.yml

==============
SCALE REPLICAS - Scale Out and Scale In
==============

Scale Out
--------

First open anotherwindows live

-- kubectl get pods --watch


-- kubectl get rs   [To list the replicasets]

-- kubectl scale rs/ib-rs --replicas=10  [Now see pods creating live]

Scale In
--------

-- kubectl scale rs/ib-rs --replicas=5  [Now see pods creating live]

LIFO: LAST IN FIRST OUT.

IF A POD IS CREATED LASTLY IT WILL DELETE FIRST WHEN SCALE IN

Note: This Scale out and in is manual, later we learn how to automate

Now, all pods are running with ib-image:latest image , but if i want to change the image to mobilebanking and update the POD, not possible in ReplicaSet

-- kubectl describe pod -l app=bank | grep -i ID   [ALl pods are using ib-image:latest]

Update the image in the replicaset, you cannot update in yml file, it will create a new replicaSet so there is a command to edit current replicaset

-- kubectl edit rs/ib-rs    [change internetbankingrepo to insurance]

-- kubectl describe pod -l app=bank | grep -i ID  [Still it shows internetbanking, image is not change , that's the problem with Replica SET, We cannot update the application]


vi replicaset.yml  -- change to insurance

-- kubectl apply -f replicaset.yml    [This will give error that ib-rs already exits. So need to create a new RS again]

-- kubectl get pods --show-labels

-- kubectl describe pod -l app=bank | grep -i ID   [you still see old image internetbanking ]

But if you scale out, new pods will contains insurance repo

-- kubectl scale rs/ib-rs --replicas=5
-- kubectl describe pod -l app=bank | grep -i ID  [you see mobilebanking . only new image are insurance.
                                                   This is the drawback of replicaset]

Using ReplicaSet we cannot roll out the application

Advantage
-- self healing
-- scaling

Drawbacks
 -- we cannot roll in and roll out, we cant update the applications using ReplicaSet, lets use DEPLOYMENT

rs ---> pods

kubectl delete rs ib-rs
~

ReplicationController:
=======================

Same as ReplicaSet. It also used for handling multiple replicas of specific pod. But it doesn't contain selector and its child field matchField. matchField where it will search for pods based on a specific label name and adds them to Cluster
                                                                                             

Kind : ReplicationController

This below code doesn't contain matchLabels Field
******************************************

apiVersion: apps/v1
kind: ReplicationController
metadata:
  name: ib-rs
  labels:
    app: bank
spec:
  replicas: 3
  template:  
    metadata:
      labels:
        app: bank
    spec:  
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest


Replication Controller
---------------------

The Replication Controller is the original form of replication in Kubernetes

The Replication Controller uses equality-based selectors to manage the pods.

The rolling-update command works with Replication Controllers

Replica Controller is deprecated and replaced by ReplicaSets.

Replica Set  
------------

ReplicaSets are a higher-level API that gives the ability to easily run multiple instances of a given pod

ReplicaSets Controller uses set-based selectors to manage the pods.

The rolling-update command won’t work with ReplicaSets.

Deployments are recommended over ReplicaSets.
